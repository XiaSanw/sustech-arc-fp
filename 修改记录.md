# 修改记录

## 01 - 任务2.3 抗干扰鲁棒性训练配置（2026-01-07）

### 问题现象
- 使用之前训练的模型测试任务2.3时，机器人一被施加外力就全部快速跌倒飞出去
- 模型完全没有抗干扰能力

### 根本原因
1. 训练时外力配置不足：
   - 外力间隔 `interval_range_s=(5.0, 10.0)` 太长
   - 触发概率 `probability=0.3` 太低
   - 实际训练中很少被推，没有学会抗干扰

2. 奖励权重配置问题：
   - 速度跟踪权重过高（`rew_lin_vel_xy=8.0`），被推时仍盲目追速度
   - 姿态稳定权重不足，没有优先恢复平衡的激励

### 解决方案：针对任务2.3的专门训练配置

#### 文件位置
`exts/bipedal_locomotion/bipedal_locomotion/tasks/locomotion/cfg/PF/limx_base_env_cfg.py`

#### 核心修改策略
- ✅ 降低速度跟踪权重（被推时优先平衡，而非追速度）
- ✅ 强化姿态稳定性惩罚
- ✅ 允许快速反应（降低动作平滑性约束）
- ✅ 训练时频繁施加强外力
- ✅ 折中处理 `pen_feet_distance`（防止训练崩溃）

---

### 具体配置修改

#### 1. 奖励权重调整（RewardsCfg类）

##### 速度跟踪（降低优先级）
```python
rew_lin_vel_xy = RewTerm(
    func=mdp.track_lin_vel_xy_exp,
    weight=4.0,  # 从 8.0 → 4.0
    params={"command_name": "base_velocity", "std": math.sqrt(0.2)}
)

rew_ang_vel_z = RewTerm(
    func=mdp.track_ang_vel_z_exp,
    weight=2.0,  # 从 4.0 → 2.0
    params={"command_name": "base_velocity", "std": math.sqrt(0.2)}
)
```

##### 姿态稳定性（强化控制）
```python
pen_flat_orientation = RewTerm(
    func=mdp.flat_orientation_l2,
    weight=-15.0  # 从 -10.0 → -15.0，强化姿态控制
)

pen_ang_vel_xy = RewTerm(
    func=mdp.ang_vel_xy_l2,
    weight=-0.1  # 从 -0.05 → -0.1，限制roll/pitch旋转
)

pen_base_height = RewTerm(
    func=mdp.base_com_height,
    params={"target_height": 0.78},
    weight=-15.0,  # 从 -20.0 → -15.0（适中）
)
```

##### 允许快速反应
```python
pen_action_rate = RewTerm(
    func=mdp.action_rate_l2,
    weight=-0.002  # 从 -0.005 → -0.002，允许激进动作
)

pen_action_smoothness = RewTerm(
    func=mdp.ActionSmoothnessPenalty,
    weight=-0.002  # 从 -0.005 → -0.002
)
```

##### 关键修改：防止训练崩溃
```python
pen_feet_distance = RewTerm(
    func=mdp.feet_distance,
    weight=-10.0,  # 从 -100 → -10.0（折中方案）
    params={
        "min_feet_distance": 0.115,
        "feet_links_name": ["foot_[RL]_Link"]
    }
)
```

##### 其他惩罚（保持原值）
```python
# 以下惩罚项保持原配置不变
pen_lin_vel_z = RewTerm(func=mdp.lin_vel_z_l2, weight=-0.5)
pen_joint_torque = RewTerm(func=mdp.joint_torques_l2, weight=-0.00008)
pen_joint_accel = RewTerm(func=mdp.joint_acc_l2, weight=-2.5e-07)
pen_joint_pos_limits = RewTerm(func=mdp.joint_pos_limits, weight=-2.0)
pen_joint_vel_l2 = RewTerm(func=mdp.joint_vel_l2, weight=-1e-03)
pen_joint_powers = RewTerm(func=mdp.joint_powers_l1, weight=-5e-04)
pen_undesired_contacts = RewTerm(func=mdp.undesired_contacts, weight=-0.5, params={...})
pen_feet_regulation = RewTerm(func=mdp.feet_regulation, weight=-0.1, params={...})
foot_landing_vel = RewTerm(func=mdp.foot_landing_vel, weight=-0.5, params={...})
```

---

#### 2. 外力配置调整（EventsCfg类）

```python
push_robot = EventTerm(
    func=mdp.apply_external_force_torque_stochastic,
    mode="interval",
    interval_range_s=(4.0, 6.0),  # 从 (5.0, 10.0) → (4.0, 6.0)，更频繁
    params={
        "asset_cfg": SceneEntityCfg("robot", body_names="base_Link"),
        "force_range": {
            "x": (-700.0, 700.0),  # 从 600 → 700N
            "y": (-700.0, 700.0),
            "z": (-0.0, 0.0),
        },
        "torque_range": {
            "x": (-70.0, 70.0),   # 从 60 → 70N⋅m
            "y": (-70.0, 70.0),
            "z": (-0.0, 0.0)
        },
        "probability": 0.5,  # 从 0.3 → 0.5，更频繁施加外力
    },
    is_global_time=False,
    min_step_count_between_reset=0,
)
```

---

### 配置对比总结

| 项目 | 原配置 | 任务2.3配置 | 说明 |
|------|--------|------------|------|
| **奖励权重** ||||
| `rew_lin_vel_xy` | 8.0 | **4.0** | 降低速度跟踪优先级 |
| `rew_ang_vel_z` | 4.0 | **2.0** | 同上 |
| `pen_flat_orientation` | -10.0 | **-15.0** | ⭐ 强化姿态稳定 |
| `pen_ang_vel_xy` | -0.05 | **-0.1** | ⭐ 限制翻滚 |
| `pen_base_height` | -20.0 | **-15.0** | 适中调整 |
| `pen_feet_distance` | -100 | **-10.0** | 🔥 防止训练崩溃 |
| `pen_action_rate` | -0.005 | **-0.002** | 允许快速反应 |
| `pen_action_smoothness` | -0.005 | **-0.002** | 同上 |
| **外力配置** ||||
| `push_robot.interval` | (5,10)s | **(4,6)s** | 更频繁推力 |
| `push_robot.probability` | 0.3 | **0.5** | 提高触发率 |
| `push_robot.force` | 600N | **700N** | 增强外力 |
| `push_robot.torque` | 60N⋅m | **70N⋅m** | 增强力矩 |

**注**：其他小惩罚项（`pen_joint_torque`, `pen_joint_accel`, `pen_undesired_contacts` 等）保持原配置不变。

---

### 训练建议

1. **清理旧日志**（可选）
   ```bash
   rm -rf logs/rsl_rl/pf_tron_1a_flat/
   ```

2. **启动训练**（3000迭代）
   ```bash
   nohup python scripts/rsl_rl/train.py \
       --task Isaac-Limx-PF-Blind-Flat-v0 \
       --headless \
       --num_envs 4096 \
       --iterations 3000 \
       > task23_train_$(date +%Y%m%d_%H%M%S).log 2>&1 &

   echo $! > train_pid.txt
   ```

3. **监控训练**
   ```bash
   # 实时查看日志
   tail -f task23_train_*.log

   # 启动 Tensorboard（可选）
   tensorboard --logdir logs/rsl_rl --port 6006 --host 0.0.0.0
   ```

4. **关注指标**
   - Episode Reward：应在 0-15 范围内波动
   - 机器人行为：被推后能快速调整姿态，允许暂时偏离速度指令
   - 训练时长：预计 2-4 小时（取决于 GPU）

5. **测试抗干扰**
   ```bash
   python scripts/rsl_rl/play.py \
       --task Isaac-Limx-PF-Blind-Flat-Demo-v0 \
       --num_envs 4 \
       --video \
       --video_length 1000
   ```

---

### 预期效果

- ✅ 训练曲线平稳，不会在2000+迭代崩溃
- ✅ 机器人被推后能快速恢复平衡，不会立即摔倒
- ✅ 在保持一定速度跟踪能力的同时，优先保证姿态稳定性
- ✅ 承受外力能力：目标 ≥700N 瞬时推力
- ✅ 3000迭代后应能看到明显的抗干扰能力

---

### 后续优化方向

如果3000迭代后效果良好，可以：
1. 继续训练到 5000-8000 迭代，进一步提升性能
2. 逐步增加 `pen_flat_orientation` 到 -20.0（增强姿态控制）
3. 增加外力到 800-1000N（测试极限鲁棒性）
4. 提高外力触发概率到 0.6-0.7

如果3000迭代后仍不稳定：
1. 降低学习率到 `5e-4` 或 `3e-4`
2. 降低外力到 500-600N，先训练稳定再逐步增强

---

## 02 - 外力与鲁棒性综合优化（2026-01-07）

### 问题发现

**训练结果**：
- 迭代2265时因数值不稳定崩溃（Value loss爆炸 → NaN）
- 但前2200迭代表现优秀：Mean reward达到140+，Episode length接近1000步

**测试问题**：
- 使用迭代2200的模型测试任务2.3时，机器人被700N推力**直接踢飞**
- 后续测试中350N推力仍然导致机器人倒地
- 700N推力对10kg机器人 ≈ 70 m/s² ≈ 7倍重力加速度，过于极端

### 根本原因分析

1. **外力强度过大**：
   - 700N对小型双足机器人是"车祸级别"冲击
   - 学术界标准：10kg级机器人抗干扰测试通常为200-400N
   - 当前配置超出合理物理场景

2. **姿态控制权重不足**：
   - 机器人被推后无法快速恢复平衡姿态
   - 需要更强的姿态稳定性和高度维持惩罚

3. **训练与测试矛盾**：
   - 训练时外力触发少（平均每episode 2-3次）
   - 模型主要学习"正常行走"，抗干扰样本不足
   - 700N推力即使训练时遇到也难以恢复

4. **速度跟踪 vs 抗干扰的平衡**：
   - 任务2.2需要稳定环境专注速度控制
   - 任务2.3需要频繁扰动学习平衡恢复
   - 如果外力过频，会牺牲速度跟踪性能

### 解决方案：外力降低 + 鲁棒性增强 + 优先级调整

#### 核心策略
- ✅ **降低外力强度**：保证物理合理性（700N → 300N）
- ✅ **提高外力频率**：增加抗干扰训练样本（probability: 0.5 → 0.6）
- ✅ **强化姿态控制**：增加平衡性相关惩罚权重
- ✅ **适当降低速度权重**：优先保证平衡，再追求速度（3.5/1.5）
- ✅ **一次训练满足两个任务**：平衡速度跟踪与抗干扰能力

---

### 具体配置修改

#### 1. 外力配置调整（EventsCfg，第377-399行）

```python
push_robot = EventTerm(
    func=mdp.apply_external_force_torque_stochastic,
    mode="interval",
    interval_range_s=(4.0, 6.0),  # 保持不变（不影响速度训练）
    params={
        "asset_cfg": SceneEntityCfg("robot", body_names="base_Link"),
        "force_range": {
            "x": (-300.0, 300.0),  # 从 700N → 300N（降低57%）
            "y": (-300.0, 300.0),
        },
        "torque_range": {
            "x": (-35.0, 35.0),    # 从 70N⋅m → 35N⋅m（降低50%）
            "y": (-35.0, 35.0),
        },
        "probability": 0.6,  # 从 0.5 → 0.6（提高20%，增加抗干扰训练频率）
    },
    is_global_time=False,
    min_step_count_between_reset=0,
)
```

#### 2. 鲁棒性权重增强（RewardsCfg类）

```python
# 强化姿态稳定性
pen_flat_orientation = RewTerm(
    func=mdp.flat_orientation_l2,
    weight=-20.0  # 从 -15.0 → -20.0，强烈惩罚姿态倾斜
)

# 强化高度维持
pen_base_height = RewTerm(
    func=mdp.base_com_height,
    params={"target_height": 0.78},
    weight=-20.0  # 从 -15.0 → -20.0，快速恢复站立高度
)

# 强化翻滚限制
pen_ang_vel_xy = RewTerm(
    func=mdp.ang_vel_xy_l2,
    weight=-0.15  # 从 -0.1 → -0.15，限制roll/pitch旋转
)
```

**速度跟踪（适当降低，优先保证平衡）**：
```python
rew_lin_vel_xy = RewTerm(
    func=mdp.track_lin_vel_xy_exp,
    weight=3.5,  # 从 4.0 → 3.5
    params={"command_name": "base_velocity", "std": math.sqrt(0.2)}
)

rew_ang_vel_z = RewTerm(
    func=mdp.track_ang_vel_z_exp,
    weight=1.5,  # 从 2.0 → 1.5
    params={"command_name": "base_velocity", "std": math.sqrt(0.2)}
)
```

**外力概率提升（增加抗干扰训练样本）**：
```python
push_robot.params["probability"] = 0.6  # 从 0.5 → 0.6
```

**其他奖励权重保持不变**：
```python
# 快速反应（已在01版调整）
pen_action_rate.weight = -0.002
pen_action_smoothness.weight = -0.002

# 其他惩罚保持原值
pen_lin_vel_z.weight = -0.5
pen_joint_torque.weight = -0.00008
pen_joint_accel.weight = -2.5e-07
pen_feet_distance.weight = -10.0
# ...
```

---

### 配置对比总结

| 项目 | 01版配置 | 02版配置 | 变化 | 理由 |
|------|---------|---------|------|------|
| **外力配置** ||||
| `push_robot.force` | 700N | **300N** | ↓ 57% | 更合理的物理强度 |
| `push_robot.torque` | 70N⋅m | **35N⋅m** | ↓ 50% | 匹配力的降低 |
| `push_robot.interval` | (4,6)s | **(4,6)s** | 不变 | 保持训练平衡 |
| `push_robot.probability` | 0.5 | **0.6** | ↑ 20% | ⭐ 增加抗干扰训练频率 |
| **鲁棒性权重** ||||
| `pen_flat_orientation` | -15.0 | **-20.0** | ↑ 33% | ⭐ 强化姿态稳定 |
| `pen_base_height` | -15.0 | **-20.0** | ↑ 33% | ⭐ 强化高度维持 |
| `pen_ang_vel_xy` | -0.1 | **-0.15** | ↑ 50% | ⭐ 限制翻滚 |
| **速度跟踪** ||||
| `rew_lin_vel_xy` | 4.0 | **3.5** | ↓ 12.5% | 降低速度优先级，优先保证平衡 |
| `rew_ang_vel_z` | 2.0 | **1.5** | ↓ 25% | 同上 |

### 物理强度分析

| 推力 | 加速度 | 相当于 | 评价 |
|------|--------|--------|------|
| 700N | 70 m/s² | 7g | ❌ 车祸级别，不合理 |
| 500N | 50 m/s² | 5g | ⚠️ 极端冲击，偏大 |
| 350N | 35 m/s² | 3.5g | ⚠️ 中等偏强，测试中仍推倒 |
| **300N** | **30 m/s²** | **3g** | ✅ **中等强度，合理** |
| 200N | 20 m/s² | 2g | ⚠️ 偏轻，挑战性不足 |

**学术界参考**：
- Boston Dynamics Atlas (75kg)换算：70-110N等效
- Unitree H1 (47kg)换算：60-100N等效
- 学术论文标准（10kg级）：200-350N为中度到重度测试

---

### 重要说明

#### Demo环境配置
- Demo环境（`Isaac-Limx-PF-Blind-Flat-Demo-v0`）继承基础配置的外力大小
- Demo特殊设置：
  - 间隔：2-3秒（更频繁展示）
  - 概率：90%（几乎每次都推）
  - **力大小：300N**（与训练环境一致）

#### 关于跳跃策略的说明
**用户问题**：机器人能否通过跳跃（高度变化、足部快速伸缩）来保持水平稳定性？

**回答：不建议，且当前配置阻止跳跃行为**

原因：
1. 当前惩罚项`pen_base_height = -20.0`和`pen_lin_vel_z = -0.5`强制机器人保持恒定高度
2. Point Foot机器人接地面积小，跳跃时双脚离地会失去支撑，更容易翻倒
3. 学术界Point Foot研究主要靠快速步态调整，而非跳跃
4. 启用跳跃需要大幅降低高度惩罚，但风险极高

#### 关于两阶段训练的说明
**用户问题**：如果先训练速度跟踪，后期增加鲁棒性权重，会影响前期的速度跟踪吗？

**回答：会！这叫"灾难性遗忘"（Catastrophic Forgetting），不建议分阶段训练**

原因：
1. 改变奖励权重 = 改变优化方向
2. 新方向的梯度更新会覆盖旧方向的网络权重
3. PPO没有机制保护之前学到的速度跟踪能力
4. **正确做法**：从一开始就用平衡配置（当前方案：速度权重适中 + 鲁棒性权重增强）

---

### 预期效果

**速度跟踪（任务2.2）**：
- 约75-80%训练时间用于速度控制（probability=0.6，略少于之前）
- 预期速度误差：0.4-0.6 m/s（相比之前略有放宽）
- Episode完成率：>85%
- **权衡策略**：略微牺牲速度跟踪精度，换取显著提升的鲁棒性

**抗干扰（任务2.3）**：
- 承受300N推力不倒
- 被推后1-2步内恢复步态
- 姿态控制更强，快速恢复平衡
- 训练样本增加20%（probability: 0.5 → 0.6），学习更充分
- 符合学术标准的中等强度测试

**训练稳定性**：
- 避免数值爆炸（配合保守的学习率）
- 预计3000迭代顺利完成
- Mean reward 可能稍低（8-15范围），因为鲁棒性权重增强且外力更频繁

---

### 训练建议

```bash
# 使用02版配置训练
nohup python scripts/rsl_rl/train.py \
    --task Isaac-Limx-PF-Blind-Flat-v0 \
    --headless \
    --num_envs 4096 \
    --iterations 3000 \
    > task23_train_v2_$(date +%Y%m%d_%H%M%S).log 2>&1 &

echo $! > train_pid.txt
```

### 后续优化

如果3000迭代后效果优秀，可选：
1. 继续训练到5000迭代，进一步提升性能
2. 测试更高强度（350-400N）的极限鲁棒性
3. 进一步增强姿态控制（`pen_flat_orientation = -25.0`）
4. 降低学习率（5e-4）并增加外力频率（probability=0.6）

如果鲁棒性仍不足：
1. 进一步降低速度跟踪权重到3.0/1.5
2. 增加更多姿态相关奖励（如足部接触稳定性）
3. 延长训练到5000-8000迭代
